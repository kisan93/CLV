{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Porter-logo.png\" alt=\"Drawing\" align=\"right\" style=\"width: 100px;\"/>\n",
    "\n",
    "# Customer Lifetime Value\n",
    "\n",
    "\n",
    "In this Notebook probabilistic models would be used to estimate CLV of Porter's customer base. The objective is to come up with a baseline CLV model which can be used a reference for comparing advanced ML models.\n",
    "For building these models transactional data from completed_spot_orders_mv table is used\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "import lifetimes as lt\n",
    "from sklearn import metrics\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "orders=pd.read_csv('CLV/orders_1619.csv') # Time period : 2016-01-01 to 2019-12-31\n",
    "orders['order_date']=pd.to_datetime(orders['order_date'],format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre processing\n",
    "\n",
    "* Removing Transacrtion with negative customer fare\n",
    "* Removing Fraudulent orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing transaction with negative customer fare\n",
    "orders=orders[orders.customer_fare>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fraudulent orders**\n",
    "\n",
    "It is possible that customer can do fraudulent transactions, to identify these we can plot the distribution of orders placed by customer in a day and find the outlier threshold.\n",
    "\n",
    "The threshold is set to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     88.863856\n",
       "2      9.108297\n",
       "3      1.505337\n",
       "4      0.357182\n",
       "5      0.105231\n",
       "6      0.035664\n",
       "7      0.015003\n",
       "8      0.003772\n",
       "9      0.002389\n",
       "10     0.002053\n",
       "11     0.000545\n",
       "12     0.000377\n",
       "13     0.000126\n",
       "14     0.000084\n",
       "20     0.000042\n",
       "15     0.000042\n",
       "Name: order_id, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=orders[orders.order_date> '2019-01-01'] # Taking most recent year\n",
    "orders_count=df.groupby(['customer_id','order_date'])['order_id'].count()\n",
    "(orders_count.value_counts()/orders_count.shape[0])*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the fraudulent orders\n",
    "orders['order_count']=orders.groupby(['customer_id','order_date'])['order_id'].transform('count')\n",
    "orders=orders[orders.order_count<6] \n",
    "orders=orders.drop('order_count',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating customer cohorts  \n",
    "\n",
    "Since we have large customer base we will segment it into various cohorts and rules are based on date of first purchase and city. Currently logic for segementing customers are soley on business intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cohort(data,city,end_date,start_date=None):\n",
    "    if start_date is None:\n",
    "        start_date=min(data['order_date'])\n",
    "    \n",
    "    temp_df=data.copy()\n",
    "    temp_df['first_purchase_dt']=temp_df.groupby('customer_id')['order_date'].transform('min')\n",
    "    cohort=temp_df.loc[(temp_df.first_purchase_dt.between(start_date,end_date)) & (temp_df.geo_region_id==city),]\n",
    "    \n",
    "    #Aggregating at a day level\n",
    "    cohort=cohort.groupby(['customer_id','order_date'])['customer_fare'].sum().reset_index()\n",
    "    \n",
    "    print('Number of customers present: {}'.format(cohort.customer_id.nunique()))\n",
    "    print('Number of orders: {}'.format(cohort.shape[0]))\n",
    "    \n",
    "    return(cohort)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of customers present: 16784\n",
      "Number of orders: 379672\n"
     ]
    }
   ],
   "source": [
    "# Selecting a particular cohort, city : Mumbai and first purchase between 2016-01 to 2016-06\n",
    "cohort1=create_cohort(orders,1,end_date='2016-06-30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing cohort data\n",
    "\n",
    "Each customer present in the cohort should be regular customer, in orders to define a regular customer following rules are followed :\n",
    "\n",
    "* Customer should place atleast minimum number of orders in observation time\n",
    "* Customer should not churn in the observation time period\n",
    "\n",
    "Let's set the observeration time period for two years and preproces the cohort data accrodingly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minimum number of orders**\n",
    "\n",
    "To find the minimum number of orders for each customer in the cohort, we will use the approach below :\n",
    "\n",
    "*min_order=Cohort purchase rate*\n",
    "\n",
    "*Cohort purchase rate = Total orders/Total customers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohort_minorder(data):\n",
    "    temp_df=data.copy()\n",
    "    val=temp_df.shape[0]/temp_df.customer_id.nunique()\n",
    "    print('minimum orders choosen for cohort:{}'.format(round(val,2)))\n",
    "    temp_df['total_orders']=temp_df.groupby('customer_id')['order_date'].transform('count')\n",
    "    temp_df=temp_df.loc[temp_df.total_orders >= val,]\n",
    "    temp_df.drop('total_orders',axis=1,inplace=True)\n",
    "    return temp_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding customers who have churned**\n",
    "\n",
    "Before building the CLV model we should assure that none of the customers has already churned within the observation time period. In order to find the churn customers we can fit emperical cumulative ditribution function to cohort's between purchase time distribution and then use various percenetile value of this distribution to determine if the customer has churned already.\n",
    "\n",
    "Please go through the following link for further explaination :\n",
    "\n",
    "__[Link to Article](https://towardsdatascience.com/modelling-customer-churn-when-churns-are-not-explicitly-observed-with-r-a768a1c919d5)__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom function for ecdf\n",
    "\n",
    "def emperical_cdf(data,threshold):\n",
    "    data=np.unique(data)\n",
    "    percentiles=[]\n",
    "    n=len(data)\n",
    "    sort_data=np.sort(data)\n",
    "    \n",
    "    for i in np.arange(1,n+1):\n",
    "        p=1-(i/n)\n",
    "        percentiles.append(p)\n",
    "    \n",
    "    ecdf=pd.DataFrame({'timediff':sort_data,'prob':percentiles})\n",
    "    return max(ecdf.loc[ecdf.prob>=threshold,'timediff'])\n",
    "\n",
    "\n",
    "\n",
    "def cohort_churn_customer(data,end_date,threshold):\n",
    "    T=dt.strptime(end_date,'%Y-%m-%d')\n",
    "    temp_df=data.copy()\n",
    "    temp_df=temp_df.sort_values(by=['customer_id','order_date'])\n",
    "    temp_df['timediff']=temp_df.groupby('customer_id')['order_date'].diff(periods=1)\n",
    "    temp_df['timediff']=temp_df.timediff.astype('timedelta64[D]')\n",
    "    temp_df=temp_df.dropna()\n",
    "    val=emperical_cdf(temp_df.timediff,threshold)\n",
    "    print('Maximum days of gap choosen for cohort : {}'.format(val))\n",
    "    temp_df['last_purchase_date']=temp_df.groupby('customer_id')['order_date'].transform('max')\n",
    "    temp_df['last_order_gap']=T-temp_df.last_purchase_date\n",
    "    temp_df['last_order_gap']=temp_df.last_order_gap.astype('timedelta64[D]')\n",
    "    temp_df['max_timegap']=temp_df.groupby('customer_id')['timediff'].transform('max')\n",
    "    temp_df=temp_df.loc[(temp_df.last_order_gap >= val) | (temp_df.max_timegap >= val)]\n",
    "    customer_ids=np.unique(temp_df.customer_id)\n",
    "    \n",
    "    return customer_ids\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the cohort_churn_customer and cohort_minorder functions\n",
    "\n",
    "def cohort_preprocess(cohort,start_date,end_date,threshold=0.80):\n",
    "    temp_df=cohort.copy()\n",
    "    #Filtering data by observation time period\n",
    "    temp_df=temp_df.loc[temp_df.order_date.between(start_date,end_date),]\n",
    "    #Finding minorder value\n",
    "    temp_df=cohort_minorder(temp_df)\n",
    "    \n",
    "    #Finding churn customers\n",
    "    customer_ids=cohort_churn_customer(temp_df,end_date,threshold)\n",
    "    #Removing churned customers\n",
    "    final=temp_df.loc[~temp_df.customer_id.isin(customer_ids),]\n",
    "    \n",
    "    print('Number of customers present: {}'.format(final.customer_id.nunique()))\n",
    "    print('Number of orders: {}'.format(final.shape[0]))\n",
    "    #print('Number of customers removed after preprocess: {}'.format(a-final.customer_id.nunique()))\n",
    "    \n",
    "    return final\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum orders choosen for cohort:13.6\n",
      "Maximum days of gap choosen for cohort : 133.0\n",
      "Number of customers present: 1443\n",
      "Number of orders: 117560\n"
     ]
    }
   ],
   "source": [
    "cohort1p=cohort_preprocess(cohort1,'2016-01-01','2017-12-31',threshold=0.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development\n",
    "\n",
    "Model development will include the following steps :\n",
    "\n",
    "* Summarizing the Transaction data into RFM data\n",
    "* Splitting summary data into calibration and holdout set\n",
    "* Model Train\n",
    "* Parameter tuning\n",
    "* Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing the data and splitting into calibration and holdout\n",
    "\n",
    "\n",
    "def summarize_cohort_split(data,calibration_end,freq=7,observation_end=None):\n",
    "    if observation_end is None:\n",
    "        observation_end=data['order_date'].max()\n",
    "    else:\n",
    "        observation_end=dt.strptime(observation_end,'%Y-%m-%d')\n",
    "    #finding number of periods in calibration and holdout data\n",
    "    \n",
    "    count_periods_c=(dt.strptime(calibration_end,'%Y-%m-%d')-data.order_date.min()).days\n",
    "    count_periods_c=int(count_periods_c/freq)\n",
    "    count_periods_h=(observation_end-dt.strptime(calibration_end,'%Y-%m-%d')).days\n",
    "    count_periods_h=int(count_periods_h/freq)\n",
    "    \n",
    "    \n",
    "    calibration_data=lifetimes.utils.summary_data_from_transaction_data(data,customer_id_col='customer_id',\n",
    "                                            datetime_col='order_date',observation_period_end=calibration_end,\n",
    "                                            datetime_format='%Y-%m-%d',freq='D',monetary_value_col='customer_fare',\n",
    "                                                                        freq_multiplier=freq)\n",
    "    \n",
    "    \n",
    "    \n",
    "    temp_df=data.copy()\n",
    "    temp_df=temp_df.loc[(temp_df.order_date > calibration_end) & (temp_df.order_date <=observation_end),]\n",
    "    holdout_data=temp_df.groupby('customer_id')['order_date'].count().reset_index()\n",
    "    holdout_data.rename(columns={'order_date':'actual'},inplace=True)\n",
    "    print('shape of calibration data {}'.format(calibration_data.shape))\n",
    "    print('shape of holdout data {}'.format(holdout_data.shape))\n",
    "    print('periods in train and test {} {}'.format(count_periods_c,count_periods_h))\n",
    "    \n",
    "    return calibration_data,holdout_data,count_periods_c,count_periods_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of calibration data (1443, 4)\n",
      "shape of holdout data (1443, 2)\n",
      "periods in train and test 52 52\n"
     ]
    }
   ],
   "source": [
    "#Splitting preprocess cohort data into calibration and holdout\n",
    "c,h,m,n=summarize_cohort_split(cohort1p,calibration_end='2016-12-31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Train**\n",
    "\n",
    "Follwoing probabilistic models are available in lifetime package :\n",
    "\n",
    "* BG/BB\n",
    "* BG/NBD\n",
    "* MBG/NBD\n",
    "* Pareto/NBD\n",
    "\n",
    "_Note : All the above models predicts exprected orders not clv, to get clv we need expected revenue and for this we can try another model on top of these or use average monetary value_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_evaluate_holdout(model_obj,count_future_time_periods,hist_freq,hist_recency,hist_T,holdoutdata):\n",
    "    val=model_obj.conditional_expected_number_of_purchases_up_to_time(count_future_time_periods,hist_freq,hist_recency,hist_T)\n",
    "    return metrics.mean_absolute_error(holdoutdata['actual'],val)\n",
    "\n",
    "\n",
    "def models_train(train,test,m,n,alpha=0.01):\n",
    "    \n",
    "    #Initilaizing the models instances\n",
    "    #bg=lt.fitters.beta_geo_beta_binom_fitter.BetaGeoBetaBinomFitter(penalizer_coef=alpha)\n",
    "    bgnbd=lt.fitters.beta_geo_fitter.BetaGeoFitter(penalizer_coef=alpha)\n",
    "    mbg=lt.fitters.modified_beta_geo_fitter.ModifiedBetaGeoFitter(penalizer_coef=alpha)\n",
    "    pareto=lt.fitters.pareto_nbd_fitter.ParetoNBDFitter(penalizer_coef=alpha)\n",
    "    \n",
    "    #Fitting in the training data\n",
    "    #bg.fit(train['frequency'],train['recency'],train['T'])\n",
    "    bgnbd.fit(train['frequency'],train['recency'],train['T'])\n",
    "    mbg.fit(train['frequency'],train['recency'],train['T'])\n",
    "    pareto.fit(train['frequency'],train['recency'],train['T'])\n",
    "    \n",
    "    #Out sample performance\n",
    "    #mae_bgbb=models_evaluate_holdout(bg,n,train['frequency'],train['recency'],train['T'],test)\n",
    "    mae_bgnbd=models_evaluate_holdout(bgnbd,n,train['frequency'],train['recency'],train['T'],test)\n",
    "    mae_mbg=models_evaluate_holdout(mbg,n,train['frequency'],train['recency'],train['T'],test)\n",
    "    mae_pareto=models_evaluate_holdout(pareto,n,train['frequency'],train['recency'],train['T'],test)\n",
    "    \n",
    "    #print('accuracy of bgbb model in holdout set {}'.format(mae_bgbb))\n",
    "    print('accuracy of bgnbd model in holdout set {}'.format(mae_bgnbd))\n",
    "    print('accuracy of mbg model in holdout set {}'.format(mae_mbg))\n",
    "    print('accuracy of pareto model in holdout set {}'.format(mae_pareto))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of bgnbd model in holdout set 19.38253428152132\n",
      "accuracy of mbg model in holdout set 19.381276879856053\n",
      "accuracy of pareto model in holdout set 18.954757229817414\n"
     ]
    }
   ],
   "source": [
    "models_train(c,h,m,n,alpha=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
